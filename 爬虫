# 不定长参数
# def getInfo(name,address,*args,**agrs2):
#     print('大家好我叫%s，我来自%s'%(name,address))
#     print(args)  #args 是一个元组类型
#     print(agrs2)  #字典数据类型
#
# getInfo('刘德华','九龙','a','b','c','d',age = 18)

'''
正则表达式：对字符串操作的一种逻辑公式，事先定义好的一些特定字符，及这些特定字符的组合，组成一个规则
字符串，这个'规则字符串'表达对字符串的一种过滤逻辑。它是匹配字符串的工具


#一般字符 abc 匹配abc
# '.'匹配除"\n"换行符以外都字符 a.c->abc
#[...]匹配字符串里面的任意字符串 a[bc]e->abe,ace
#\匹配特殊字符 a\.c->a.c a\\c->a\c
#预定字符集
#\d 数字:{0-9} a\dc-> a2c a3c
\D 非数字 a\Dc-> abc aec
\s空白字符(<空格>\t\r\n\f\v) a\sc a c
\S非空白字符 a|Sc abc
\w 单词字符{A-Z.a-z,0-9} a\wc abc
\W 非单词字符 a\Wc a c
数量词
*匹配前一个政府0或者无限次 abc* ab abccccc abccc
+匹配前一个字符1次或者无限次 abc+ abc abcccc
? 匹配前一个字符0次或一次 abc? ab abc
{m}匹配前一个字符m次 ab{2}c abbc
{m,n}匹配前一个字符m到n次 ab{1,2}c abc abbc
'''

import re

# comple()编译正则表达式，返回一个对象模式
# re1=re.compile('\w{1}\w')


# print(type(re1))


# match()
# 从字符串开头匹配，成功返回一个匹配对象，失败返回None
# print(re.match('hello','hello haha').group())

#serach 函数在字符串内查找模式匹配，只要找到第一个匹配返回，没有匹配
#返回None
# res=re.compile('[a-zA-Z]{1}')
# strs='123abc456'
# print(re.search(res,strs).group())

#findall()遍历匹配，可以获得字符串所有匹配字符串，返回一个列
# pa=re.compile('[a-zA-z]{1}')
# pa=re.compile('[a-z][a-z][a-z]')#返回满足a,b,c
# pa=re.compile('([a-z])[a-z]([a-z])')#返回分组部分及()
#
# strs='123abc456'
# print(re.findall(pa,strs))

#finditer()返回一个顺序访问每一个匹配结果(Match对象)的迭代器，找到满足的所有子串，并作为迭代器返回
#返回完整和分组匹配结果
# # pa=re.compile('[a-zA-z]{1}')
# pa=re.compile('([a-z])[a-z]([a-z])')
# strs='123abc456def789'
# result=pa.finditer(strs)
# print(re.finditer(pa,strs))#上面两个表示相同，均为迭代器<callable_iterator object at 0x000001CAF3C809E8>
# print(result)
# for i in result:
#     print(i)#<_sre.SRE_Match object; span=(3, 6), match='abc'>
#     #match使用group()
#     print(i.group())
#     print(i.group(1))#返回每一个的首位
#split()按照能够匹配的子串将string分割后返回列表
# print(re.split('\d+','one11two2three3four4five5'))
# print(re.split('\W+','one11two2three3four4five5'))
#
# str1='one,two,three,four'
# str2='one1two2three3'
# pattern=re.compile('\W+')
# print(re.split(pattern,str1))
# # print(re.split(pattern,str2))
# '''
# sub返回替换,匹配后替换
# '''
# # pattern=re.compile('\d')
# # str1='one11two2three3four4five5'
# # print(re.sub(pattern,'...',str1))
# # print(pattern.sub('-',str1))
# #返回匹配结果以及替换次数
# # print(re.subn(pattern,'...',str1))
#
# '''
#     引用分组
#
# '''
# str='hello 123,world 321'
# pattern=re.compile('(\w+) (\d+)')
# print(pattern.finditer(str))#返回一个对象
# # for i in pattern.finditer(str):
# #     print(i.group(0))#全部和分组一，分组二
# #     print(i.group(1))
# #     print(i.group(2))
#
# # print(pattern.sub(r'\2 \1',str))#取得分组 \1 \2,用r转一下
# # print(pattern.sub(r'\2****\1',str))#取得分组 \1 \2,用r转一下
#
# '''
# 贪婪与非贪婪
# 贪婪：在整个表达式匹配成功的前提下，尽可能多的匹配
# 非贪婪：在整个表达式匹配成功的前提下，尽可能少的匹配
# '''

# str1='aaa<p>hello</p>bbb<p>world</p>ccc'
# pattern=re.compile('<p>\w+</p>')
#
# print(pattern.findall(str1))
#贪婪模式
# pattern=re.compile('<p>.*</p>')
# print(pattern.findall(str1))
# #['<p>hello</p>bbb<p>world</p>']全部拿来
# #非贪婪模式
# pattern=re.compile('<p>.*?</p>')#？只匹配一次
# print(pattern.findall(str1))#['<p>hello</p>', '<p>world</p>']

# '''
# 匹配中文字符
# '''
# str1='你好,hello,帅哥'
# #中文加英文
# pattern=re.compile('\w+')
#
# #只匹配中文
# pattern=re.compile(('[\u4e00-\u9fa5]+'))
# # ['你好', '帅哥']
# print(pattern.findall(str1))

#1.将下列url匹配出来
# str='''
# # 313156566@qq.com
# # hsagsa@163.com
# # http://www.sas.com
# # http://www.asa.com.cn
# # ftps://www.sss.org
# # '''
# # pattern=re.compile('[a-z]+:\/\/www(\.\w+)(\.\w+){1,2}')
# # str1=pattern.finditer(str)
# # for i in str1:
# #     print(i.group())




# #判断一下字符是否为中文
# str="广东省s广州市"
#
# pattern=re.compile('[\u4e00-\u9fa5]')
# print(pattern.findall(str))
# #如果不是中文字符开始且结尾，匹配不出来
# pattern=re.compile('^[\u4e00-\u9fa5]+$')
# print(pattern.findall(str))
# if pattern.findall(str):
#     print('全是中文字符')
# else:
#     print('不全是中文字符')


#一个正则表达式过滤网页上js脚本(把scrpt过滤掉)
# script='以下哎内容不显示:<script ssaddada </script>'
# pattern=re.compile('<script.*?</script>')
# print(pattern.findall((script)))
# #替换掉
# print(pattern.sub('',script))

# phone='13888886666'
# #变成138****6666
# pattern=re.compile('^(1[3578]\d)(\d{4})(\d{4})$')
# phone2=re.finditer(pattern,phone)
# for i in phone2:
#     print(i.group())
#     print(i.group(1))
#     print(i.group(2))
#     print(i.group(3))
# #记住，要加r
# print(pattern.sub(r'\1***\3',phone))

'''
爬虫
'''
#####
# 使用BeautifulSuop爬虫
####
# #-*- coding:utf-8 -*-
from bs4 import BeautifulSoup
import requests
# #content及utf-8解码
# r=requests.get('http://www.baidu.com').content.decode('utf-8')
# #html.parser与lxml解析方式,解析成文档对象
# soup=BeautifulSoup(r,'html.parser')
# # print(type(soup))
# #格式美化,横变竖
# html=soup.prettify()
# print(html)


###读取一个文件
soup=BeautifulSoup(open('web.html','r',encoding='utf-8'),'lxml')
# print(type(soup))

#TAG拿html文件
# print(soup.html)
# print(soup.html.head)
# #通过标签获取
# print(soup.html.meta)
#默认bs对象第一个div
# print(soup.div)
# print(soup.div.input)
#tag name sttrs
# #标签的属性,name属性，
# print(soup.div.input.name)返回标签名
# print(soup.div.input.name.attrs)返回标签里面的属性，是字典格式
# print(soup.div.input.name.attrs['type'])属性


#拿logo内容NavigabelString
#通过string获取标签内容,即使注释也可以拿到
#String:获取多个内容，不过遍历获取
# print(soup.div.span.string)


# '''
# 遍历文档树，直接子节点
# '''
#渠道各个子树
# print(soup.div.contents)
# print(soup.div.contents[0])
# print(soup.div.contents[1])
#获取属性
# print(soup.div.contents[2].attrs['type'])


# #一个迭代对象迭代器, content则是一个列表
# print(soup.div.children)
# #可迭代对象,生成器
# print(soup.div.descendants)

#拿取节点内容
# print(soup.div.contents[1].string)
# print(soup.div.contents[1].text)
#查看父节点
# print(soup.div.contents[1].parent)
# #全部父节点,迭代器
# print(soup.div.contents[1].parents)


#查看兄弟节点,可以一直next_sibling拿下一个兄弟
# print(soup.div)
# print(soup.div.next_sibling.next_sibling)
#查看上一个兄弟
# print(menu.previous_sibling)


#查找文档树
# print(soup.find_all('div'))#查找所有的div
# print(soup.find_all('a'))#查找所有的a
# result=soup.find_all('a')
# for i in result:
#     print(i.text)#拿到文本
#     print(i.attrs['href'])#拿到链接


# #找到标签(通过列表)
# print(soup.find_all((['a','span'])))
# #传标签名
# print(soup.find_all('name'))
# #通过id找
# print(soup.find_all(id='menu'))
# #通过关键字
# print(soup.find_all(id='one',attrs={'name':'aa'}))
#
# #Text文本
# print(soup.find_all(text='要闻'))
#
# #find查找一个
# print(soup.find(id='menu'))
# print(soup.find(id='menu').find_all('a',limit=2))#限制找两次


# -*- coding:utf-8 -*-
'''
获取Python100练习
'''
from bs4 import BeautifulSoup
import requests

url='http://www.runoob.com/python/python-100-examples.html'
headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'}
#发送请求
r=requests.get(url,headers=headers).content.decode('utf-8')
# print(r)

#解析文档
soup=BeautifulSoup(r,'lxml')
# print(soup)

#查找每个练习的a链接href属性对应链接地址
re_a=soup.find(id='content').ul.find_all('a')

#创建一个列表保存url
lis=[]
#找其属性，'href'
for i in re_a:
    lis.append(i.attrs['href'])
# print(lis)

'''
2.根据获取每个链接地址请求每个练习内容
'''
#请求详细页面
ar=requests.get('http://www.runoob.com'+lis[0],headers=headers).content.decode('utf-8')
# print(ar)
#解析html文档
soup_ar=BeautifulSoup(ar,'lxml')
# print(type(soup_ar))
#查找标题
title=soup_ar.find(id='content').h1.text
#查找题目
tm=soup_ar.find(id='content').h1.next_sibling.next_sibling.next_sibling.next_sibling.text
# print(tm)
#获取源代码
code=soup_ar.find(class_='hl-main').text
print(code)

# #保存文件
# with open('py-100.csv','a+',encoding='utf-8')as files:
#     files.write(dic['title']+'\n')

#d导出csv格式
# import pandas as pd
# data.to_csv('py-100.csv')


#id#号，class.获取




'''
css选择器
'''
#soup=BeautifulSoup(open(html,r,encoding='utf-8'),'lxml')
#获取所有span 所有a标签
#print(soup.select('span'))#返回列表
#print(soup.select(('a')))


#通过class类名进行查找 .类名,返回列表
#print(soup.select('.one'))

#通过id进行查找
#print(soup.select('#menu'))

#通过属性查找比如第二个，在span标签下的class='f'
#print(soup.select('a[name='aa']'))
#print(soup.select('span[class='f']'))


#div中id包含s（比较生僻）
#print(soup.select('div[id~="s"]'))

#组合查找查找div下面class=one
#print(soup.select('div .one'))
#查找class=right下的class=one的
#print(soup.select('.right .one'))

#id=header的下一个兄弟id=clear下一个兄弟的class=menu下面标签为a
#print(soup.select('#header + .clear + #menu a'))

#选择a标签的第二个元素
#print(soup.select('a:nth-of-type(2)'))
#获取id=content的直接子标签（儿子，不能是孙子）
#print(soup.select('#content > a'))

#获取id=header所有兄弟的div
#print(soup.select('#header ~ div'))

#获取所有内容（有[0]所以第一个）
print(soup.select('#menu a')[0].get_text())#获取文本内容
print(soup.select('#menu a')[0].attrs['href'])#获取属性
print(soup.select('#menu a')[0].text)#获取文本内容
print(soup.select('#menu a')[0]['href'])#获取属性


















#-*-conding:utf-8-*-

from lxml import etree
#html=etree.HTML(open(html,encoding='utf-8').read())


#解析文档
#将html文档进行序列化
#result=etree.tostring(html,pretty_print=True,encoding='utf-8').decode('utf-8')

#Xpath文档查找信息语言)

#节点关系，选择节点
#/从根节点开始选取
#/html/body/div
#选取节点
#dom=html.xpath('/html/body/div')
#//匹配当前结点开始选择，返回文档所有a标签，返回的是一个列表
#dom=html.xpath('//a')
#dom[0]
#.获取当前节点
#..获取当前节点父节点
#@选取属性
#获取标签a中class='one'的text文档
#全部文档
# dom=html.xpath('//a[@class="one"]/text()')
# print(dom)
#第一个文档
# dom=html.xpath('string(//a[@class="one"])')
#获取属性
#print(html.xpat('//a[@class="one"]/@href'))







